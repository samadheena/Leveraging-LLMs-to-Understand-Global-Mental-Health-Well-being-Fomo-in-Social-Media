# Leveraging-LLMs-to-Understand-Global-Mental-Health-Well-being-Fomo-in-Social-Media
Table of Content
1. Overview
2. Inference Settings
3. Datasets
4. Models
5. Results
6.Fine-tuning Hyperparamters
In this work, we present the first comprehensive evaluation of multiple LLMs, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4, on various mental health prediction tasks via online text data. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously.
CONTRIBUTIONS
Inference Settings
Zero-shot Prompting

Few-shot Prompting
, where 
 denotes # of demonstrations

Prompt Designs
Prompt Designs
![prompt_designs](https://github.com/samadheena/Leveraging-LLMs-to-Understand-Global-Mental-Health-Well-being-Fomo-in-Social-Media/assets/159759911/c4c24224-cb57-44f5-b780-08f65100f933)



