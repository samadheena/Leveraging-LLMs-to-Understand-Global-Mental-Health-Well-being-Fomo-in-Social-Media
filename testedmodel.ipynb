{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITfIGyNYffmH",
        "outputId": "c9f1b09f-aca3-4ff3-9d42-d6d2de4e9f12"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftConfig, PeftModel\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_3v6DMcffmK"
      },
      "source": [
        "# Mental Mistral SFT on our dataset\n",
        "\n",
        " - DO NOT RUN till ***********"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsqiUpLuffmL",
        "outputId": "7f42b5db-7bf8-48a5-f10a-a777694df714"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.01s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32000, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm()\n",
              "            (post_attention_layernorm): MistralRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "adapter = \"GRMenon/mental-health-mistral-7b-instructv0.2-finetuned-V2\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model,\n",
        "    add_bos_token=True,\n",
        "    trust_remote_code=True,\n",
        "    padding_side='left'\n",
        ")\n",
        "\n",
        "# Create peft model using base_model and finetuned adapter\n",
        "config = PeftConfig.from_pretrained(adapter)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
        "                                             load_in_4bit=True,\n",
        "                                             device_map=device,\n",
        "                                             torch_dtype='auto')\n",
        "model = PeftModel.from_pretrained(model, adapter)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz9QeXSKffmM"
      },
      "source": [
        "### Preparing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT7zW9QkffmM",
        "outputId": "1243af8c-1cb8-4ffe-82a4-0ab5ccba9dc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'chosen', 'rejected', 'category', 'text'],\n",
              "    num_rows: 2365\n",
              "})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def prepare_sample_text(example):\n",
        "    instruction   = \"Read the user query and give solutions that can be implemented\"\n",
        "    text = f\"\"\"<s>[INST] {instruction} \\\\n user:{example['prompt']} [/INST] \\\\n {example['chosen']} </s>\"\"\"\n",
        "    return {'text': text}\n",
        "\n",
        "def chars_token_ratio(dataset, tokenizer, nb_examples=None):\n",
        "    \"\"\"\n",
        "    Estimate the average number of characters per token in the dataset.\n",
        "    If nb_examples is None, use the whole dataset.\n",
        "    \"\"\"\n",
        "    if nb_examples is None:\n",
        "        nb_examples = len(dataset)\n",
        "    total_characters, total_tokens = 0, 0\n",
        "    for example in tqdm(dataset, total=nb_examples):\n",
        "        text = example['text']\n",
        "        total_characters += len(text)\n",
        "        if tokenizer.is_fast:\n",
        "            total_tokens += len(tokenizer(text).tokens())\n",
        "        else:\n",
        "            total_tokens += len(tokenizer.tokenize(text))\n",
        "    return total_characters / total_tokens\n",
        "\n",
        "# Specify the path to your local CSV file here\n",
        "csv_file_path = \"data/train.csv\"\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "dataset = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files=csv_file_path,\n",
        "    split='train',\n",
        ")\n",
        "\n",
        "# Assuming the CSV file has 'question' and 'response_j' columns,\n",
        "# you need to prepare the text for each example in the dataset.\n",
        "dataset = dataset.map(prepare_sample_text)\n",
        "\n",
        "# Now you can use the `dataset` object as needed, for example:\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOvCoijhffmN"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.train_test_split(test_size=236, seed=42)\n",
        "train_data = dataset['train']\n",
        "valid_data = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYRK7hHLffmN",
        "outputId": "d72a18f9-aedb-42f1-c12d-47643801920e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2129/2129 [00:01<00:00, 1633.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Characters per token: 4.469672672948612\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "chars_per_token = chars_token_ratio(train_data, tokenizer)\n",
        "print(f\"Characters per token: {chars_per_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mj3iJc7uffmO"
      },
      "outputs": [],
      "source": [
        "from trl.trainer import ConstantLengthDataset\n",
        "\n",
        "train_dataset=ConstantLengthDataset(\n",
        "    tokenizer,\n",
        "    train_data,\n",
        "    formatting_func=prepare_sample_text,\n",
        "    infinite=True,\n",
        "    seq_length=1024,\n",
        "    chars_per_token=chars_per_token,\n",
        ")\n",
        "\n",
        "valid_dataset=ConstantLengthDataset(\n",
        "    tokenizer,\n",
        "    valid_data,\n",
        "    formatting_func=prepare_sample_text,\n",
        "    infinite=False,\n",
        "    seq_length=1024,\n",
        "    chars_per_token=chars_per_token,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuRnSHcSffmO",
        "outputId": "e67dd922-2fd4-4d30-bd38-600f4df07901"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ApAP87IffmP"
      },
      "source": [
        "# ********************************************************************************************************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHKJTCREffmQ"
      },
      "source": [
        "# DPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD24oh4QffmQ"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# del sft_trainer, base_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHvCuoXoffmQ"
      },
      "outputs": [],
      "source": [
        "def get_device_map() -> str:\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "device = get_device_map()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYeZ0kYBffmR"
      },
      "outputs": [],
      "source": [
        "def return_prompt_and_responses(samples)-> dict[str,str]:\n",
        "    return {\n",
        "        \"prompt\":[\n",
        "            \"<s> [INST] Read the user query and give solutions that can be implemented \\n user:\" + question + \" [/INST] </s>\" for question in samples[\"prompt\"]\n",
        "        ],\n",
        "        \"chosen\": samples[\"chosen\"],\n",
        "        \"rejected\": samples[\"rejected\"],\n",
        "    }\n",
        "\n",
        "\n",
        "def get_dataset_paired(data_file=\"data/train.csv\", sanity_check=False, cache_dir=None, num_proc=4):\n",
        "    \"\"\"\n",
        "    Loads the dataset from a CSV file, processes it, and optionally performs a sanity check by selecting a subset.\n",
        "    \"\"\"\n",
        "    # Load the dataset from a CSV file\n",
        "    dataset = load_dataset('csv', data_files=data_file, cache_dir=cache_dir)['train']\n",
        "\n",
        "    # Perform a sanity check if requested, to work with a smaller subset of the data\n",
        "    if sanity_check:\n",
        "        dataset = dataset.select(range(min(len(dataset), 1000)))\n",
        "\n",
        "    # Process the dataset to format the prompts and responses\n",
        "    processed_dataset = dataset.map(\n",
        "        return_prompt_and_responses,\n",
        "        batched=True,\n",
        "        num_proc=num_proc,\n",
        "        remove_columns=dataset.column_names,  # Remove original columns\n",
        "    )\n",
        "\n",
        "    return processed_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5KUYhLOffmR",
        "outputId": "4e313737-9a94-4cc7-8661-90aa9168308a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.08it/s]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "adapter = \"GRMenon/mental-health-mistral-7b-instructv0.2-finetuned-V2\"\n",
        "\n",
        "config = PeftConfig.from_pretrained(adapter)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
        "                                             torch_dtype=torch.float16)\n",
        "model = PeftModel.from_pretrained(model, adapter)\n",
        "model=model.merge_and_unload()\n",
        "model.save_pretrained(\"new_model/final_merged_checkpoint\", safe_serialization=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbR__PHWffmR",
        "outputId": "46ddc95e-a766-4d9a-98d3-e5e256dc7f89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.68s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.71s/it]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model=AutoModelForCausalLM.from_pretrained(\n",
        "    \"new_model/final_merged_checkpoint\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "\n",
        "model_ref=AutoModelForCausalLM.from_pretrained(\n",
        "    \"new_model/final_merged_checkpoint\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3KCT_wkffmS"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOdUuKSiffmS"
      },
      "outputs": [],
      "source": [
        "train_dataset = get_dataset_paired('data/train.csv')\n",
        "train_dataset = train_dataset.filter(lambda x: len(x[\"prompt\"]) + len(x[\"chosen\"]) <= 1024 and len(x[\"prompt\"]) + len(x[\"rejected\"]) <= 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s2HwFP_ffmS"
      },
      "outputs": [],
      "source": [
        "eval_dataset = get_dataset_paired('data/train.csv')\n",
        "eval_dataset = eval_dataset.filter(lambda x: len(x[\"prompt\"]) + len(x[\"chosen\"]) <= 1024 and len(x[\"prompt\"]) + len(x[\"rejected\"]) <= 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOXeJADmffmS"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from peft import LoraConfig, TaskType\n",
        "from trl import DPOTrainer\n",
        "import os\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuvcE-G6ffmS"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    save_steps=10,\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    max_steps=100,\n",
        "    save_strategy=\"no\",\n",
        "    logging_steps=1,\n",
        "    output_dir=\"new_model/\",\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    warmup_steps=10,\n",
        "    fp16=True,\n",
        "    report_to=\"wandb\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=20,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUfG6-H-ffmT",
        "outputId": "24505a94-f89a-45a4-df0a-eac5591883d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parameter 'function'=<bound method DPOTrainer.tokenize_row of <trl.trainer.dpo_trainer.DPOTrainer object at 0x7f642ba4f1f0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
          ]
        }
      ],
      "source": [
        "# Create DPO trainer\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model,\n",
        "    model_ref,\n",
        "    args=training_args,\n",
        "    beta=0.1,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=peft_config,\n",
        "    max_prompt_length=512,\n",
        "    max_length=1024,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2hZEkBvffmT",
        "outputId": "b03a8215-623d-43f0-f215-da8a4e4a9b85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvinayakkgarg8599\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/vinayakkgarg8599/wandb/run-20240321_150136-jt8c1t3w</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vinayakkgarg8599/huggingface/runs/jt8c1t3w' target=\"_blank\">prime-totem-4</a></strong> to <a href='https://wandb.ai/vinayakkgarg8599/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/vinayakkgarg8599/huggingface' target=\"_blank\">https://wandb.ai/vinayakkgarg8599/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/vinayakkgarg8599/huggingface/runs/jt8c1t3w' target=\"_blank\">https://wandb.ai/vinayakkgarg8599/huggingface/runs/jt8c1t3w</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:228: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "/home/vinayakkgarg8599/llm_env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 57:30, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rewards/chosen</th>\n",
              "      <th>Rewards/rejected</th>\n",
              "      <th>Rewards/accuracies</th>\n",
              "      <th>Rewards/margins</th>\n",
              "      <th>Logps/rejected</th>\n",
              "      <th>Logps/chosen</th>\n",
              "      <th>Logits/rejected</th>\n",
              "      <th>Logits/chosen</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.019700</td>\n",
              "      <td>0.040451</td>\n",
              "      <td>2.212070</td>\n",
              "      <td>-4.341268</td>\n",
              "      <td>0.987097</td>\n",
              "      <td>6.553337</td>\n",
              "      <td>-141.697311</td>\n",
              "      <td>-106.962601</td>\n",
              "      <td>-2.666264</td>\n",
              "      <td>-2.865987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>0.029679</td>\n",
              "      <td>-0.065412</td>\n",
              "      <td>-10.351833</td>\n",
              "      <td>0.991067</td>\n",
              "      <td>10.286421</td>\n",
              "      <td>-201.802979</td>\n",
              "      <td>-129.737427</td>\n",
              "      <td>-2.317307</td>\n",
              "      <td>-2.342934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.008600</td>\n",
              "      <td>0.012977</td>\n",
              "      <td>1.282627</td>\n",
              "      <td>-10.239790</td>\n",
              "      <td>0.994045</td>\n",
              "      <td>11.522417</td>\n",
              "      <td>-200.682556</td>\n",
              "      <td>-116.257034</td>\n",
              "      <td>-2.308453</td>\n",
              "      <td>-2.397155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.011181</td>\n",
              "      <td>1.327407</td>\n",
              "      <td>-10.762150</td>\n",
              "      <td>0.993548</td>\n",
              "      <td>12.089558</td>\n",
              "      <td>-205.906158</td>\n",
              "      <td>-115.809212</td>\n",
              "      <td>-2.320941</td>\n",
              "      <td>-2.422301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.019100</td>\n",
              "      <td>0.010252</td>\n",
              "      <td>1.315000</td>\n",
              "      <td>-10.822784</td>\n",
              "      <td>0.994045</td>\n",
              "      <td>12.137784</td>\n",
              "      <td>-206.512497</td>\n",
              "      <td>-115.933296</td>\n",
              "      <td>-2.322066</td>\n",
              "      <td>-2.423676</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=0.0728387291051331, metrics={'train_runtime': 3458.0458, 'train_samples_per_second': 0.463, 'train_steps_per_second': 0.029, 'total_flos': 0.0, 'train_loss': 0.0728387291051331, 'epoch': 0.79})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dpo_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lP0rR_ucffmT",
        "outputId": "d75a7b85-8a6b-48dd-9c25-4d841f6605c1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt': '<s> [INST] Read the user query and give solutions that can be implemented \\n user:I feel really alone lately. [/INST] </s>',\n",
              " 'chosen': \"Loneliness can be tough, but there are ways to feel more connected. Have you tried reaching out to old friends or family you haven't spoken to in a while? Maybe there's a local book club or hiking group you could join to meet new people who share your interests.  Going for a walk in a nearby park or nature reserve can also be a great way to boost your mood and feel more connected to the world around you. If you're feeling overwhelmed, there are also many self-help books on overcoming loneliness, like Loneliness: The Hidden Link to Mental Health and Sociability by John Cacioppo or The Power of Vulnerability by Brené Brown.  Consider checking them out from your local library!\",\n",
              " 'rejected': 'Everyone feels lonely sometimes, just get out there more.'}"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJFHtni8ffmT",
        "outputId": "d0204bed-7930-4a17-dfbf-8dd40cb1a48c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'chosen', 'rejected'],\n",
              "    num_rows: 1940\n",
              "})"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36XrodsaffmT",
        "outputId": "57a600bb-2081-41b1-a407-652eb1a8daee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Mar 21 13:47:12 2024       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    52W / 400W |  23419MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A     37061      C   ...rg8599/llm_env/bin/python    23288MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-vIJ3xOffmT",
        "outputId": "6c8084b5-210c-4370-ead3-1ae02f9a4c2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVKdsENoffmT"
      },
      "outputs": [],
      "source": [
        "dpo_trainer.model.save_pretrained(\"final_model/model_mar22_1am\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xe2V0lI2ffmU",
        "outputId": "0cc882b8-bfb5-416d-8844-ec0ccb110fd0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.48it/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftConfig, PeftModel\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"new_model/final_merged_checkpoint\", return_dict=True, torch_dtype=torch.float16)\n",
        "model = PeftModel.from_pretrained(model, \"final_model/model_mar22_1am\")\n",
        "model.eval()\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained(\"dpo-mistralai-7b-mental-health/final_merged_checkpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vj6ABloFffmU",
        "outputId": "e779346f-e45e-484c-ccbf-47c446689d5c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.52s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32000, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm()\n",
              "            (post_attention_layernorm): MistralRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model,\n",
        "    add_bos_token=True,\n",
        "    trust_remote_code=True,\n",
        "    padding_side='left'\n",
        ")\n",
        "\n",
        "# Create peft model using base_model and finetuned adapter\n",
        "config = PeftConfig.from_pretrained(\"final_model/model_mar22_1am\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
        "                                             load_in_4bit=True,\n",
        "                                             device_map='auto',\n",
        "                                             torch_dtype='auto')\n",
        "model = PeftModel.from_pretrained(model, \"final_model/model_mar22_1am\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJr3i48UffmU",
        "outputId": "5fafeaa1-ff6b-4b02-b2f9-4529209388ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Cyberbullying can be devastating and isolating. It's important to remember that you are not alone and there are resources available to help. Consider reaching out to a trusted adult, such as a teacher or counselor, to discuss what's happening and develop a plan for addressing the bullying behavior. Additionally, there are online resources available through organizations such as StopBullying.gov and Cyberbullying.org that can provide support and guidance.</s>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Network error (ReadTimeout), entering retry loop.\n"
          ]
        }
      ],
      "source": [
        "instruction   = \"Read the user query and give solutions that can be implemented. Share only the relevant resources with contact details.\"\n",
        "text = \"I'm being cyberbullied at school. These kids keep sending me nasty messages and leaving me out of online groups. They make me feel invisible and worthless. I don't know what I did to deserve this, and I don't know how to make it stop.\"\n",
        "prompt = f\"\"\"{instruction}\n",
        "[INST] user:{text} [/INST]\"\"\"\n",
        "inputs = tokenizer( prompt, return_tensors=\"pt\")\n",
        "\n",
        "# generate output\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"),max_new_tokens=1024)\n",
        "assessment = tokenizer.batch_decode(outputs)[0]\n",
        "op_string = assessment.split(\"[/INST]\")[1]\n",
        "print(op_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMVPm-ENffmU"
      },
      "source": [
        "## Sample tests:\n",
        "\n",
        "- Some of my friends abuse me publicly on facebook. I am weak and cannot fight them.\n",
        "- I'm sorry to hear that you are experiencing bullying on Facebook. It's important to remember that social media platforms should be a safe space for connection and positivity. If you are feeling weak or unable to fight back against the bullying, consider reaching out to trusted friends or family members for support. You can also report the bullying to Facebook and seek resources from organizations such as Stop Bullying.org. Remember, you are not alone and there are people who care about you.</s>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd10tK8dffmU"
      },
      "source": [
        "- I enjoy reading other people's tweets and have a lot of friends on dacebook. However, I don't get too many likes on Instagram and that makes me sad.\n",
        "-  It's understandable that receiving likes on Instagram can impact your mood. Consider focusing on authentic connections on social media platforms, engaging with others' posts through thoughtful comments, and remembering that likes are not the only measure of meaningful interactions.</s>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHkVP8FJffmU"
      },
      "source": [
        "- My instagram photos get a lot of likes but the same people who liked my photo ignore me in school.\n",
        "- It's important to remember that social media interactions don't always reflect real-life relationships. If you're feeling ignored or disconnected from people who like your Instagram photos, consider reaching out to them in person or through private messaging to start conversations and build connections. Remember that authentic relationships are built on trust, respect, and open communication.</s>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zwCbU3zffmU"
      },
      "source": [
        "- My ex boyfriend is not deleting my naked pictures. I requested him many times but he is threatening to leak them online. I am super tensed and scared.\n",
        "-  I'm sorry to hear about your distressing situation. It's important to remember that you have the right to control your own images and to request that explicit content be deleted. If your ex is refusing to comply, consider reaching out to law enforcement or a digital security expert for assistance. Additionally, counseling or therapy can help you process your emotions and develop coping strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYnASmYYffmU"
      },
      "source": [
        "- My ex boyfriend is not deleting my naked pictures. I requested him many times but he is threatening to leak them online. I am super tensed and scared.\n",
        "-  I'm sorry to hear about your distressing situation. It's important to remember that you have the right to control your own images and to request that explicit content be deleted. If your ex-boyfriend is refusing to comply, consider reaching out to a trusted friend or family member for support. Additionally, there are resources available to help you navigate this situation, such as the National Domestic Violence Hotline (1-800-799-7233) or the Cyber Civil Rights Organization (www.cybercivilrights.org). Remember, you are not alone and there are people and organizations ready to help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBGFaL69ffmU"
      },
      "source": [
        "- I had a lot of friends in my childhood and I am still connected to them on Facebook. However, I am not able to speak to any new people now. Why is this happening?\n",
        "-  This is a concerning issue. It's important to address your feelings of isolation and explore ways to connect with new people. Consider joining local groups or clubs related to your interests, attending networking events, or utilizing online platforms to meet new individuals. Counseling can also be beneficial in addressing underlying causes of social isolation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhUMiyBTffmV"
      },
      "source": [
        "- I spend hours and hours scrolling on my instagram reels. I forgot to water my plants yesterday. I don't know what's happening.\n",
        "-  It sounds like you are experiencing a disconnect between your online and offline worlds. Here are some resources that might help:1. Mindfulness practices: Mindfulness meditation can help you stay present and focused on the moment.2. Digital Detox: Consider setting boundaries around your social media use and scheduling time for activities offline.3. Therapy: A therapist can help you explore your relationship with technology and develop coping strategies.4. Plants and Nature: Consider incorporating plants into your space as a reminder of nature and a way to bring calm to your environment.5. Contact: Reach out to friends and family for support and connection offline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-C8h7YlffmV"
      },
      "source": [
        "- Someone is threatening to kill my family on Facebook. We are deeply tensed and disturbed.\n",
        "-  I'm sorry to hear about the threatening messages on Facebook. Your safety and that of your family is paramount. Consider reporting the messages to Facebook and contacting local law enforcement for assistance. Additionally, consider reaching out to a counselor or therapist to process this experience and cope with any resulting emotions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKIzfXL3ffmh"
      },
      "source": [
        "- I'm being cyberbullied at school. These kids keep sending me nasty messages and leaving me out of online groups. They make me feel invisible and worthless. I don't know what I did to deserve this, and I don't know how to make it stop.\n",
        "-  Cyberbullying can be devastating and isolating. It's important to remember that you are not alone and there are resources available to help. Consider reaching out to a trusted adult, such as a teacher or counselor, to discuss what's happening and develop a plan for addressing the bullying behavior. Additionally, there are online resources available through organizations such as StopBullying.gov and Cyberbullying.org that can provide support and guidance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQlq0GbIffmi"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(\"gargvinayakk99/dpo-mistralai-7b-mental-health\", token=\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqQ5tNb2ffmi",
        "outputId": "aaee5b9e-7e7a-4f20-9952-208ce530c4ac"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywidgets` module: `pip install ipywidgets`.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[0;32m~/llm_env/lib/python3.10/site-packages/huggingface_hub/_login.py:236\u001b[0m, in \u001b[0;36mnotebook_login\u001b[0;34m(new_session, write_permission)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipywidgets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwidgets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mwidgets\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notebook_login\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnotebook_login\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/llm_env/lib/python3.10/site-packages/huggingface_hub/_login.py:239\u001b[0m, in \u001b[0;36mnotebook_login\u001b[0;34m(new_session, write_permission)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `notebook_login` function can only be used in a notebook (Jupyter or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Colab) and you need the `ipywidgets` module: `pip install ipywidgets`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m     )\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_session \u001b[38;5;129;01mand\u001b[39;00m _current_token_okay(write_permission\u001b[38;5;241m=\u001b[39mwrite_permission):\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser is already logged in.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mImportError\u001b[0m: The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywidgets` module: `pip install ipywidgets`."
          ]
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdXu0jLPffmi",
        "outputId": "b40056d4-33bb-4238-bddc-0810956b37f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.10/site-packages (8.1.1)\n",
            "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (8.20.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.9 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (4.0.9)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /opt/conda/lib/python3.10/site-packages (from ipywidgets) (3.0.9)\n",
            "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
            "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
            "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSQJh6tGffmj",
        "outputId": "c2c28c38-11cd-4322-9afb-10b154accce9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/home/vinayakkgarg8599/hf_models/dpo-mistralai-7b-mental-health/tokenizer_config.json',\n",
              " '/home/vinayakkgarg8599/hf_models/dpo-mistralai-7b-mental-health/special_tokens_map.json',\n",
              " '/home/vinayakkgarg8599/hf_models/dpo-mistralai-7b-mental-health/tokenizer.json')"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.save_pretrained(\"/home/vinayakkgarg8599/hf_models/dpo-mistralai-7b-mental-health/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPy10b-iffmj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm_env",
      "language": "python",
      "name": "llm_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}